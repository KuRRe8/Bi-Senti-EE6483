{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Product Reviews\n",
    "\n",
    "build a learning-based classifier to classify the sentiments of product reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "## for path compatibility, if you are not running from app.py, please specify the project root path as working directory (there is no __file__ in jupyter notebook)\n",
    "_root_path_ = 'D:\\\\_work\\\\Bi-Senti-EE6483'\n",
    "\n",
    "if '_root_path_' in locals():\n",
    "    os.chdir(_root_path_)\n",
    "assert os.path.basename(os.getcwd()) == 'Bi-Senti-EE6483'\n",
    "\n",
    "import configparser as cp\n",
    "\n",
    "# storage for multiple data processing, for comparison of different methods\n",
    "from enum import IntFlag, auto\n",
    "class F(IntFlag): # data flow\n",
    "    train = auto()\n",
    "    test = auto()\n",
    "    val = auto()\n",
    "    preset1 = train | test | val\n",
    "    preset2 = auto()\n",
    "    preset3 = auto()\n",
    "\n",
    "#****************************************************************************************************\n",
    "# USER DEFINED HERE\n",
    "dataflows = [\n",
    "    [],\n",
    "    [],\n",
    "    []\n",
    "]\n",
    "#****************************************************************************************************\n",
    "\n",
    "class DATA_CONTAINER(list):\n",
    "    def __init__(self, dataflows):\n",
    "        super().__init__([ [] for _ in range(len(dataflows)) ])\n",
    "    def append(self, data):\n",
    "        raise NotImplementedError('Append porhibited. Can only change sublists')\n",
    "container = DATA_CONTAINER(dataflows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in data\n",
    "\n",
    "Sure we need to read the data into our program, the most commonly used library for reading data must be **pandas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "\n",
    "# Read the data\n",
    "labeled_df = pd.read_json('data/train.json')\n",
    "unlabeled_df = pd.read_json('data/test.json')\n",
    "\n",
    "# Output the info\n",
    "print(\"\\nTrain DataFrame info:\")\n",
    "labeled_df.info()\n",
    "print(\"\\nTest DataFrame info:\")\n",
    "unlabeled_df.info()\n",
    "\n",
    "for i in range(len(container)):\n",
    "    container[i] = [deepcopy(labeled_df), deepcopy(unlabeled_df)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "\n",
    "### Preprocessing\n",
    "\n",
    "The data is not always clean and ready to use, we need to preprocess it before we can use it. The most common preprocessing steps include:\n",
    "\n",
    "- Removing useless characters\n",
    "- Tokenization\n",
    "- Removing stopwords\n",
    "- Lemmatization\n",
    "- Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for all sentences, we first apply regular expression to remove all special characters\n",
    "import re\n",
    "\n",
    "def re_removal(text: str) -> str:\n",
    "    text=re.sub('(<.*?>)', ' ', text)\n",
    "    text=re.sub('[,\\.!?:()\"]', '', text)\n",
    "    text=re.sub('[^a-zA-Z\"]',' ',text)\n",
    "    return text.lower()\n",
    "\n",
    "# tokenizer is a function that splits a text(very long str) into words(list of str)\n",
    "def tokenize(text: str, method: str) -> list:\n",
    "    if method == 'split':\n",
    "        return text.split()\n",
    "    elif method == 'nltk':\n",
    "        import nltk\n",
    "        from nltk.tokenize import word_tokenize\n",
    "        nltk.download('punkt')\n",
    "        return word_tokenize(text)\n",
    "    elif method == 'spacy':\n",
    "        import spacy\n",
    "        nlp_en_model = spacy.load(\"en_core_web_sm\")\n",
    "        return [token.text for token in nlp_en_model(text)]\n",
    "    elif method == 'gensim':\n",
    "        import gensim\n",
    "        return gensim.utils.simple_preprocess(text)\n",
    "    elif method == 'bert':\n",
    "        from transformers import BertTokenizer\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        return tokenizer.tokenize(text)\n",
    "    else:\n",
    "        raise ValueError('method not supported')\n",
    "        \n",
    "def remove_stopwords(text: list, method: str) -> list:\n",
    "    if method == 'nltk':\n",
    "        import nltk\n",
    "        from nltk.corpus import stopwords\n",
    "        nltk.download('stopwords')\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        return [word for word in text if word not in stop_words]\n",
    "    elif method == 'spacy':\n",
    "        import spacy\n",
    "        nlp_en_model = spacy.load(\"en_core_web_sm\")\n",
    "        return [token.text for token in nlp_en_model(text) if not token.is_stop]\n",
    "    else:\n",
    "        raise ValueError('method not supported')\n",
    "\n",
    "def lematize(text: str, method: str) -> list:\n",
    "    if method == 'nltk':\n",
    "        import nltk\n",
    "        from nltk.stem import WordNetLemmatizer\n",
    "        nltk.download('wordnet')\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        return [lemmatizer.lemmatize(word) for word in text]\n",
    "    elif method == 'spacy':\n",
    "        import spacy\n",
    "        nlp_en_model = spacy.load(\"en_core_web_sm\")\n",
    "        return [token.lemma_ for token in nlp_en_model(text)]\n",
    "    else:\n",
    "        raise ValueError('method not supported')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding\n",
    "\n",
    "When we get the data, they are characters rather than numbers, so we need to convert them. This procedure is called embedding.\n",
    "\n",
    "- Word2Vec\n",
    "- GloVe\n",
    "- FastText\n",
    "- BERT\n",
    "- etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, FastText\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "def get_embeddings(texts, method='word2vec'):\n",
    "    if method == 'word2vec':\n",
    "        model = Word2Vec(sentences=texts, vector_size=100, window=5, min_count=1, workers=4)\n",
    "        embeddings = [model.wv[text] for text in texts]\n",
    "    elif method == 'glove':\n",
    "        glove_file = 'data/glove.6B.100d.txt'\n",
    "        glove_model = KeyedVectors.load_word2vec_format(glove_file, binary=False, no_header=True)\n",
    "        embeddings = [[glove_model[word] for word in text if word in glove_model] for text in texts]\n",
    "    elif method == 'fasttext':\n",
    "        model = FastText(sentences=texts, vector_size=100, window=5, min_count=1, workers=4)\n",
    "        embeddings = [model.wv[text] for text in texts]\n",
    "    elif method == 'bert':\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        model = BertModel.from_pretrained('bert-base-uncased')\n",
    "        embeddings = []\n",
    "        for text in texts:\n",
    "            inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "            outputs = model(**inputs)\n",
    "            embeddings.append(outputs.last_hidden_state.mean(dim=1).detach().numpy())\n",
    "    else:\n",
    "        raise ValueError('method not supported')\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "\n",
    "#texts = [tokenize(re_removal(review), method='split') for review in labeled_df['reviews']]\n",
    "#embeddings = get_embeddings(texts, method='word2vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traditional Machine Learning Models\n",
    "\n",
    "- SVM\n",
    "- Extreme Learning Machine （One layer neural network）\n",
    "- Gaussian Process\n",
    "- Random Forest/ XGBoost/ LightGBM (Not Included)\n",
    "- Linear (Not Included)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning Models\n",
    "\n",
    "- RNN\n",
    "- UniRNN\n",
    "- LSTM\n",
    "- BiLSTM\n",
    "- GRU\n",
    "- Bert\n",
    "- Roberta\n",
    "- DistilBert\n",
    "- Albert\n",
    "- etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def model_train(method: str, train_test_data: pd.DataFrame):\n",
    "    if method == 'SVM':\n",
    "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "        from sklearn.svm import SVC\n",
    "        from sklearn.pipeline import make_pipeline\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(train_test_data['reviews'], train_test_data['sentiment'], test_size=0.2, random_state=42)\n",
    "        model = make_pipeline(TfidfVectorizer(), SVC())\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "    elif method == 'ELM':\n",
    "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        from skelm import ELM\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(train_test_data['reviews'], train_test_data['sentiment'], test_size=0.2, random_state=42)\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        X_train = vectorizer.fit_transform(X_train)\n",
    "        X_test = vectorizer.transform(X_test)\n",
    "        model = ELM(X_train.shape[1], X_train.shape[1])\n",
    "        model.add_neurons(100, 'sigm')\n",
    "        model.train(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "    elif method == 'GaussianProcess':\n",
    "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "        from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "        from sklearn.gaussian_process.kernels import RBF\n",
    "        from sklearn.pipeline import make_pipeline\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(train_test_data['reviews'], train_test_data['sentiment'], test_size=0.2, random_state=42)\n",
    "        model = make_pipeline(TfidfVectorizer(), GaussianProcessClassifier(kernel=RBF()))\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "    elif method == 'RNN':\n",
    "        from keras.preprocessing.text import Tokenizer\n",
    "        from keras.preprocessing.sequence import pad_sequences\n",
    "        from keras.models import Sequential\n",
    "        from keras.layers import Embedding, LSTM, Dense\n",
    "        from keras.optimizers import Adam\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(train_test_data['reviews'], train_test_data['sentiment'], test_size=0.2, random_state=42)\n",
    "        tokenizer = Tokenizer()\n",
    "        tokenizer.fit_on_texts(X_train)\n",
    "        X_train = tokenizer.texts_to_sequences(X_train)\n",
    "        X_test = tokenizer.texts_to_sequences(X_test)\n",
    "        X_train = pad_sequences(X_train, padding='post')\n",
    "        X_test = pad_sequences(X_test, padding='post')\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(len(tokenizer.word_index)+1, 100, input_length=X_train.shape[1]))\n",
    "        model.add(LSTM(100))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "        model.fit(X_train, y_train, epochs=10, batch_size=64)\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred = [1 if pred > 0.5 else 0 for pred in y_pred]\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "    elif method == 'UniRNN':\n",
    "        from keras.preprocessing.text import Tokenizer\n",
    "        from keras.preprocessing.sequence import pad_sequences\n",
    "        from keras.models import Sequential\n",
    "        from keras.layers import Embedding, LSTM, Dense\n",
    "        from keras.optimizers import Adam\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(train_test_data['reviews'], train_test_data['sentiment'], test_size=0.2, random_state=42)\n",
    "        tokenizer = Tokenizer()\n",
    "        tokenizer.fit_on_texts(X_train)\n",
    "        X_train = tokenizer.texts_to_sequences(X_train)\n",
    "        X_test = tokenizer.texts_to_sequences(X_test)\n",
    "        X_train = pad_sequences(X_train, padding='post')\n",
    "        X_test = pad_sequences(X_test, padding='post')\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(len(tokenizer.word_index)+1, 100, input_length=X_train.shape[1]))\n",
    "        model.add(LSTM(100, return_sequences=True))\n",
    "        model.add(LSTM(100))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "        model.fit(X_train, y_train, epochs=10, batch_size=64)\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred = [1 if pred > 0.5 else 0 for pred in y_pred]\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "    elif method == 'LSTM':\n",
    "        from keras.preprocessing.text import Tokenizer\n",
    "        from keras.preprocessing.sequence import pad_sequences\n",
    "        from keras.models import Sequential\n",
    "        from keras.layers import Embedding, LSTM, Dense\n",
    "        from keras.optimizers import Adam\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(train_test_data['reviews'], train_test_data['sentiment'], test_size=0.2, random_state=42)\n",
    "        tokenizer = Tokenizer()\n",
    "        tokenizer.fit_on_texts(X_train)\n",
    "        X_train = tokenizer.texts_to_sequences(X_train)\n",
    "        X_test = tokenizer.texts_to_sequences(X_test)\n",
    "        X_train = pad_sequences(X_train, padding='post')\n",
    "        X_test = pad_sequences(X_test, padding='post')\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(len(tokenizer.word_index)+1, 100, input_length=X_train.shape[1]))\n",
    "        model.add(LSTM(100, return_sequences=True))\n",
    "        model.add(LSTM(100, return_sequences=True))\n",
    "        model.add(LSTM(100))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "        model.fit(X_train, y_train, epochs=10, batch_size=64)\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred = [1 if pred > 0.5 else 0 for pred in y_pred]\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "    elif method == 'BiLSTM':\n",
    "        from keras.preprocessing.text import Tokenizer\n",
    "        from keras.preprocessing.sequence import pad_sequences\n",
    "        from keras.models import Sequential\n",
    "        from keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "        from keras.optimizers import Adam\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(train_test_data['reviews'], train_test_data['sentiment'], test_size=0.2, random_state=42)\n",
    "        tokenizer = Tokenizer()\n",
    "        tokenizer.fit_on_texts(X_train)\n",
    "        X_train = tokenizer.texts_to_sequences(X_train)\n",
    "        X_test = tokenizer.texts_to_sequences(X_test)\n",
    "        X_train = pad_sequences(X_train, padding='post')\n",
    "        X_test = pad_sequences(X_test, padding='post')\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(len(tokenizer.word_index)+1, 100, input_length=X_train.shape[1]))\n",
    "        model.add(Bidirectional(LSTM(100)))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "        model.fit(X_train, y_train, epochs=10, batch_size=64)\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred = [1 if pred > 0.5 else 0 for pred in y_pred]\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "    elif method == 'GRU':\n",
    "        from keras.preprocessing.text import Tokenizer\n",
    "        from keras.preprocessing.sequence import pad_sequences\n",
    "        from keras.models import Sequential\n",
    "        from keras.layers import Embedding, GRU, Dense\n",
    "        from keras.optimizers import Adam\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(train_test_data['reviews'], train_test_data['sentiment'], test_size=0.2, random_state=42)\n",
    "        tokenizer = Tokenizer()\n",
    "        tokenizer.fit_on_texts(X_train)\n",
    "        X_train = tokenizer.texts_to_sequences(X_train)\n",
    "        X_test = tokenizer.texts_to_sequences(X_test)\n",
    "        X_train = pad_sequences(X_train, padding='post')\n",
    "        X_test = pad_sequences(X_test, padding='post')\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(len(tokenizer.word_index)+1, 100, input_length=X_train.shape[1]))\n",
    "        model.add(GRU(100))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "        model.fit(X_train, y_train, epochs=10, batch_size=64)\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred = [1 if pred > 0.5 else 0 for pred in y_pred]\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "    elif method == 'BERT':\n",
    "        from transformers import BertTokenizer, BertModel\n",
    "        import torch\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(train_test_data['reviews'], train_test_data['sentiment'], test_size=0.2, random_state=42)\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        model = BertModel.from_pretrained('bert-base-uncased')\n",
    "        X_train = [tokenizer(review, return_tensors='pt', padding=True, truncation=True) for review in X_train]\n",
    "        X_test = [tokenizer(review, return_tensors='pt', padding=True, truncation=True) for review in X_test]\n",
    "        X_train = [model(**inputs).last_hidden_state.mean(dim=1).detach().numpy() for inputs in X_train]\n",
    "        X_test = [model(**inputs).last_hidden_state.mean(dim=1).detach().numpy() for inputs in X_test]\n",
    "        model = SVC()\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "    elif method == 'RoBERTa':\n",
    "        from transformers import RobertaTokenizer, RobertaModel\n",
    "        import torch\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(train_test_data['reviews'], train_test_data['sentiment'], test_size=0.2, random_state=42)\n",
    "        tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "        model = RobertaModel.from_pretrained('roberta-base')\n",
    "        X_train = [tokenizer(review, return_tensors='pt', padding=True, truncation=True) for review in X_train]\n",
    "        X_test = [tokenizer(review, return_tensors='pt', padding=True, truncation=True) for review in X_test]\n",
    "        X_train = [model(**inputs).last_hidden_state.mean(dim=1).detach().numpy() for inputs in X_train]\n",
    "        X_test = [model(**inputs).last_hidden_state.mean(dim=1).detach().numpy() for inputs in X_test]\n",
    "        model = SVC()\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "    elif method == 'DistillBert':\n",
    "        from transformers import DistilBertTokenizer, DistilBertModel\n",
    "        import torch\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(train_test_data['reviews'], train_test_data['sentiment'], test_size=0.2, random_state=42)\n",
    "        tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "        model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "        X_train = [tokenizer(review, return_tensors='pt', padding=True, truncation=True) for review in X_train]\n",
    "        X_test = [tokenizer(review, return_tensors='pt', padding=True, truncation=True) for review in X_test]\n",
    "        X_train = [model(**inputs).last_hidden_state.mean(dim=1).detach().numpy() for inputs in X_train]\n",
    "        X_test = [model(**inputs).last_hidden_state.mean(dim=1).detach().numpy() for inputs in X_test]\n",
    "        model = SVC()\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "    else:\n",
    "        raise ValueError('method not supported')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentiment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
