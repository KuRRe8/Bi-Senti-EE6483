{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Product Reviews\n",
    "\n",
    "build a learning-based classifier to classify the sentiments of product reviews\n",
    "\n",
    "Please note that this Jupyter notebook is intended to explain the thought process of this assignment, but the actual running efficiency or debugging may not be satisfactory. If you want to smoothly execute the script, you can use the full. py file in the same level directory. The content is completely consistent with the content of this document, but there are no additional markdowns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "## for path compatibility, if you are not running from app.py, please specify the project root path as working directory (there is no __file__ in jupyter notebook)\n",
    "_root_path_ = 'D:\\\\_work\\\\Bi-Senti-EE6483'\n",
    "\n",
    "if '_root_path_' in locals():\n",
    "    os.chdir(_root_path_)\n",
    "assert os.path.basename(os.getcwd()) == 'Bi-Senti-EE6483'\n",
    "\n",
    "import configparser as cp\n",
    "\n",
    "# storage for multiple data processing, for comparison of different methods\n",
    "from enum import IntFlag, auto\n",
    "class F(IntFlag): # data flow\n",
    "\n",
    "    RE = auto() # need Regex to remove special characters\n",
    "\n",
    "    TOKEN_NLTK = auto() # need tokenization by non-deeplearning methods\n",
    "    TOKEN_SPACY = auto()\n",
    "    TOKEN_GENSIM = auto()\n",
    "\n",
    "    STOPWORDS_NLTK = auto() # need stopwords removal by non-deeplearning methods\n",
    "    STOPWORDS_SPACY = auto()\n",
    "    STOPWORDS_GENSIM = auto()\n",
    "    \n",
    "    LEMMATIZE_NKTK = auto() # need lemmatization by non-deeplearning methods\n",
    "    LEMMATIZE_SPACY = auto()\n",
    "    LEMMATIZE_TEXTBLOB = auto()\n",
    "\n",
    "    EMBEDDING_WORD2VEC_TRAIN = auto() # shallow neural network training\n",
    "    EMBEDDING_WORD2VEC_PRETRAIN = auto()\n",
    "    EMBEDDING_GLOVE_PRETRAIN = auto()\n",
    "    EMBEDDING_TFIDF = auto()\n",
    "\n",
    "    MODEL_SVM = auto() # traditional machine learning\n",
    "    MODEL_ELM = auto()\n",
    "    MODEL_GP = auto() # gaussian process\n",
    "    MODEL_RF = auto() # random forest not supported yet\n",
    "    MODEL_LINEAR = auto() # OLS/ Lasso/ Ridge may not perform well in this case, not implemented\n",
    "    MODEL_RNN = auto() # deep neural network training\n",
    "    MODEL_LSTM = auto()\n",
    "    MODEL_GRU= auto()\n",
    "\n",
    "    ENSEMBLE_BERT = auto() # Fine-tuning pre-trained huggingface models, does not follow the taskflow definition, will be done separately\n",
    "    ENSEMBLE_DISTILBERT = auto()\n",
    "    ENSEMBLE_ROBERTA = auto()\n",
    "\n",
    "    CUSTOMIZED = auto() # for customized encoder-only model training\n",
    "\n",
    "\n",
    "    preset1 = RE | TOKEN_SPACY | STOPWORDS_SPACY | LEMMATIZE_SPACY | EMBEDDING_WORD2VEC_PRETRAIN | MODEL_ELM\n",
    "    preset2 = ENSEMBLE_BERT\n",
    "\n",
    "#****************************************************************************************************\n",
    "# USER DEFINED HERE\n",
    "taskflows:list[F] = [\n",
    "    F.ENSEMBLE_BERT,\n",
    "    #[F.ENSEMBLE_DISTILBERT],\n",
    "    F.preset1\n",
    "]\n",
    "#****************************************************************************************************\n",
    "\n",
    "class DATA_CONTAINER(list):\n",
    "    def __init__(self, taskflows):\n",
    "        super().__init__([ [] for _i in range(len(taskflows)) ]) # empty datacontainers with the amount of taskflows\n",
    "    def append(self, data):\n",
    "        raise NotImplementedError('Append porhibited. Can only change sublists')\n",
    "container_traintest = DATA_CONTAINER(taskflows) # for training and testing data, empty for now\n",
    "container_pred = DATA_CONTAINER(taskflows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in data\n",
    "\n",
    "Sure we need to read the data into our program, the most commonly used library for reading data must be **pandas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "\n",
    "# Read the data\n",
    "labeled_df = pd.read_json('data/train.json')\n",
    "unlabeled_df = pd.read_json('data/test.json')\n",
    "\n",
    "# Output the info\n",
    "print(\"\\nTrain DataFrame info:\")\n",
    "labeled_df.info()\n",
    "print(\"\\nTest DataFrame info:\")\n",
    "unlabeled_df.info()\n",
    "\n",
    "for i in range(len(container_traintest)):\n",
    "    container_traintest[i].append(deepcopy(labeled_df))\n",
    "    container_pred[i].append(deepcopy(unlabeled_df))\n",
    "\n",
    "assert type(container_traintest[0][0]) == pd.DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "\n",
    "### Preprocessing\n",
    "\n",
    "The data is not always clean or ready to use, we need to preprocess it before we can use it. The most common preprocessing steps include:\n",
    "\n",
    "- Removing useless characters\n",
    "- Tokenization\n",
    "- Removing stopwords\n",
    "- Lemmatization\n",
    "- Stemming (not involved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for all sentences, we first apply regular expression to remove all special characters\n",
    "import re\n",
    "\n",
    "def re_removal(text: str) -> str:\n",
    "    text=re.sub('(<.*?>)', ' ', text)\n",
    "    text=re.sub('[,\\.!?:()\"]', '', text)\n",
    "    text=re.sub('[^a-zA-Z\"]',' ',text)\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "# tokenizer is a function that splits a text(very long str) into words(list of str)\n",
    "def tokenize(text: str , method: str) -> list[str] :\n",
    "    if method == 'split':\n",
    "        return text.split()\n",
    "    elif method == 'nltk':\n",
    "        import nltk\n",
    "        from nltk.tokenize import word_tokenize\n",
    "        nltk.download('punkt')\n",
    "        return word_tokenize(text)\n",
    "    elif method == 'spacy':\n",
    "        import spacy\n",
    "        nlp_en_model = spacy.load(\"en_core_web_sm\")\n",
    "        return [token.text for token in nlp_en_model(text)] #nlp_en_model(text) returns a generator（doc), yeilds tokens, token.text is the word, token.lemma_ is the lemma, token.pos_ is the POS\n",
    "    elif method == 'gensim':\n",
    "        import gensim\n",
    "        return gensim.utils.simple_preprocess(text)\n",
    "    elif method == 'bert':\n",
    "        raise ValueError('bert based tokenizer should be implemented afterwards, since it returns a different type of data')\n",
    "    else:\n",
    "        raise ValueError('method not supported')\n",
    "        \n",
    "def remove_stopwords(text: list[str] , method: str) -> list[str]:\n",
    "    '''\n",
    "    text: for nltk, only a sentence contains words, not a list of sentences\n",
    "        for spacy, it accept a str\n",
    "        for gensim, it accept a str\n",
    "    return: for nltk and spacy, a list of filtered words\n",
    "            for gensim, a str\n",
    "    '''\n",
    "    if method == 'nltk':\n",
    "        import nltk\n",
    "        from nltk.corpus import stopwords\n",
    "        nltk.download('stopwords')\n",
    "        nltk.download('punkt')\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        return [word for word in text if word not in stop_words]\n",
    "    elif method == 'spacy':\n",
    "        import spacy\n",
    "        nlp_en_model = spacy.load(\"en_core_web_sm\")\n",
    "        stop_words = {word for word in nlp_en_model.Defaults.stop_words}\n",
    "        return [word for word in text if word not in stop_words]\n",
    "    elif method == 'gensim':\n",
    "        from gensim.parsing.preprocessing import STOPWORDS\n",
    "        stopwords = set(STOPWORDS)\n",
    "        return [word for word in text if word not in stop_words]\n",
    "    else:\n",
    "        raise ValueError('method not supported')\n",
    "\n",
    "def lematize(text: list[str], method: str) -> list[str]:\n",
    "    '''\n",
    "    text: a list of words\n",
    "    return: a list of lemmatized words\n",
    "    '''\n",
    "    if method == 'nltk':\n",
    "        import nltk\n",
    "        from nltk.stem import WordNetLemmatizer\n",
    "        def pos_tagger(nltk_tag):\n",
    "            from nltk.corpus import wordnet\n",
    "            if nltk_tag.startswith('J'):\n",
    "                return wordnet.ADJ\n",
    "            elif nltk_tag.startswith('V'):\n",
    "                return wordnet.VERB\n",
    "            elif nltk_tag.startswith('N'):\n",
    "                return wordnet.NOUN\n",
    "            elif nltk_tag.startswith('R'):\n",
    "                return wordnet.ADV\n",
    "            else:         \n",
    "                return None\n",
    "        def tagged_lemma(listofstr: list[str]) -> list[str]:\n",
    "            nonlocal lemmatizer\n",
    "            pos_tagged = nltk.pos_tag(listofstr)\n",
    "            wordnet_tagged = list(map(lambda x: (x[0], pos_tagger(x[1])), pos_tagged))\n",
    "            lemmatized_sentence = []\n",
    "            for word, tag in wordnet_tagged:\n",
    "                if tag is None:\n",
    "                    lemmatized_sentence.append(word)\n",
    "                else:       \n",
    "                    lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "            return lemmatized_sentence\n",
    "        nltk.download('wordnet')\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        return tagged_lemma(text)\n",
    "    elif method == 'spacy':\n",
    "        import spacy\n",
    "        nlp_en_model = spacy.load(\"en_core_web_sm\")\n",
    "        return [token.lemma_ for token in nlp_en_model(\" \".join(text))]\n",
    "    elif method == 'textblob':\n",
    "        from textblob import TextBlob\n",
    "        blob = TextBlob(\" \".join(text))\n",
    "        return [word.lemmatize() for word in blob.words]\n",
    "    else:\n",
    "        raise ValueError('method not supported')\n",
    "\n",
    "for taski in range(len(taskflows)):\n",
    "    if taskflows[taski] & F.RE:\n",
    "        container_traintest[taski].append(container_traintest[taski][0]['reviews'].apply(re_removal))\n",
    "        container_pred[taski].append(container_pred[taski][0]['reviews'].apply(re_removal))\n",
    "    \n",
    "    if taskflows[taski] & F.TOKEN_NLTK: # from this step, we will append each result to the container\n",
    "        container_traintest[taski].append(container_traintest[taski][-1].apply(tokenize, method='nltk'))\n",
    "        container_pred[taski].append(container_pred[taski][-1].apply(tokenize, method='nltk'))\n",
    "    elif taskflows[taski] & F.TOKEN_SPACY:\n",
    "        container_pred[taski].append(container_pred[taski][-1].apply(tokenize, method='spacy'))\n",
    "        container_traintest[taski].append(container_traintest[taski][-1].apply(tokenize, method='spacy'))\n",
    "    elif taskflows[taski] & F.TOKEN_GENSIM:\n",
    "        container_traintest[taski].append(container_traintest[taski][-1].apply(tokenize, method='gensim'))\n",
    "        container_pred[taski].append(container_pred[taski][-1].apply(tokenize, method='gensim'))\n",
    "    # after this step, last item in container is list[str] for each sentence\n",
    "    \n",
    "    if taskflows[taski] & F.STOPWORDS_NLTK:\n",
    "        container_traintest[taski].append(pd.Series(container_traintest[taski][-1]).apply(remove_stopwords, method='nltk'))\n",
    "        container_pred[taski].append(pd.Series(container_pred[taski][-1]).apply(remove_stopwords, method='nltk'))\n",
    "    elif taskflows[taski] & F.STOPWORDS_SPACY:\n",
    "        container_traintest[taski].append(pd.Series(container_traintest[taski][-1]).apply(remove_stopwords, method='spacy'))\n",
    "        container_pred[taski].append(pd.Series(container_pred[taski][-1]).apply(remove_stopwords, method='spacy'))\n",
    "    elif taskflows[taski] & F.STOPWORDS_GENSIM:\n",
    "        container_traintest[taski].append(pd.Series(container_traintest[taski][-1]).apply(remove_stopwords, method='gensim'))\n",
    "        container_pred[taski].append(pd.Series(container_pred[taski][-1]).apply(remove_stopwords, method='gensim'))\n",
    "\n",
    "    if taskflows[taski] & F.LEMMATIZE_NKTK:\n",
    "        container_traintest[taski].append(pd.Series(container_traintest[taski][-1]).apply(lematize, method='nltk'))\n",
    "        container_pred[taski].append(pd.Series(container_pred[taski][-1]).apply(lematize, method='nltk'))\n",
    "    elif taskflows[taski] & F.LEMMATIZE_SPACY:\n",
    "        container_traintest[taski].append(pd.Series(container_traintest[taski][-1]).apply(lematize, method='spacy'))\n",
    "        container_pred[taski].append(pd.Series(container_pred[taski][-1]).apply(lematize, method='spacy'))\n",
    "    elif taskflows[taski] & F.LEMMATIZE_TEXTBLOB:\n",
    "        container_traintest[taski].append(pd.Series(container_traintest[taski][-1]).apply(lematize, method='textblob'))\n",
    "        container_pred[taski].append(pd.Series(container_pred[taski][-1]).apply(lematize, method='textblob'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding\n",
    "\n",
    "When we get the data, they are characters rather than numbers, so we need to convert them. This procedure is called embedding.\n",
    "\n",
    "- Word2Vec\n",
    "- GloVe\n",
    "- FastText\n",
    "- TF-IDF (can be regarded as a kind of sentence embedding rather than word embedding)\n",
    "- Context-based Word Embedding (BERT, GPT-2, etc.) not involved here, since those method will have a different embedding type in python, it will be done afterwards.\n",
    "- etc.\n",
    "\n",
    "note: when using word embedding method rather than TF-IDF, utilizing ML models need further manipulation, we simply use the average of word vectors to represent the sentence.\n",
    "(dimension of this method is much smaller than TF-IDF,approximately 300<-->10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, FastText\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from typing import TypeAlias\n",
    "T_embedding: TypeAlias = list[list[np.ndarray]] #  corpus ,sentences, words-> embedding\n",
    "train_w2v_model = None # avoid deconstruction\n",
    "word2vec_model = None\n",
    "glove_model = None\n",
    "\n",
    "def get_embeddings(texts: list[list[str]], method: str) -> T_embedding:\n",
    "    if method == 'word2vec_trained':\n",
    "        train_w2v_model = Word2Vec(sentences=texts, vector_size=100, window=5, min_count=1, workers=4)\n",
    "        embeddings = [[train_w2v_model.wv[word] for word in text if word in train_w2v_model.wv] for text in texts]\n",
    "    elif method == 'word2vec_fromPretrained':\n",
    "        word2vec_file = 'temp/GoogleNews-vectors-negative300.bin'\n",
    "        word2vec_model = KeyedVectors.load_word2vec_format(word2vec_file, binary=True)\n",
    "        embeddings = [[word2vec_model[word] for word in text if word in word2vec_model] for text in texts]\n",
    "    elif method == 'glove_fromPretrained':\n",
    "        glove_model = api.load('glove-wiki-gigaword-100')\n",
    "        embeddings = [[glove_model[word] for word in text if word in glove_model] for text in texts]\n",
    "    elif method == 'fasttext':\n",
    "        raise NotImplementedError()\n",
    "    elif method == 'bert':\n",
    "        raise UserWarning('Bert has dynamic embedding, not like word2vec, fasttext, glove')\n",
    "    elif method == 'tfidf':\n",
    "        # when using this, make sure that 'del tfidf_vectorizer' before a new dataflow\n",
    "        if not 'tfidf_vectorizer' in locals():\n",
    "            tfidf_vectorizer = TfidfVectorizer()\n",
    "            X = tfidf_vectorizer.fit_transform(texts)\n",
    "            embeddings = X.toarray()\n",
    "        else:\n",
    "            embeddings = tfidf_vectorizer.transform(texts).toarray()\n",
    "    else:\n",
    "        raise ValueError('method not supported')\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "for taski in range(len(taskflows)):\n",
    "    if taskflows[taski] & F.EMBEDDING_WORD2VEC_TRAIN:\n",
    "        method='word2vec_trained'\n",
    "        container_traintest[taski].append(get_embeddings(container_traintest[taski][-1].tolist(), method=method))\n",
    "        container_pred[taski].append(get_embeddings(container_pred[taski][-1].tolist(), method=method))\n",
    "    elif taskflows[taski] & F.EMBEDDING_WORD2VEC_PRETRAIN:\n",
    "        method='word2vec_fromPretrained'\n",
    "        container_traintest[taski].append(get_embeddings(container_traintest[taski][-1].tolist(), method=method))\n",
    "        container_pred[taski].append(get_embeddings(container_pred[taski][-1].tolist(), method=method))\n",
    "    elif taskflows[taski] & F.EMBEDDING_GLOVE_PRETRAIN:\n",
    "        method='glove_fromPretrained'\n",
    "        container_traintest[taski].append(get_embeddings(container_traintest[taski][-1].tolist(), method=method))\n",
    "        container_pred[taski].append(get_embeddings(container_pred[taski][-1].tolist(), method=method))\n",
    "\n",
    "del train_w2v_model\n",
    "del word2vec_model\n",
    "del glove_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traditional Machine Learning Models\n",
    "\n",
    "- SVM\n",
    "- Extreme Learning Machine （One layer neural network）\n",
    "- Gaussian Process\n",
    "- Random Forest/ XGBoost/ LightGBM (Not Included)\n",
    "- Linear (Not Included)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning Models\n",
    "\n",
    "- RNN (multi-layer Elman RNN in Pytorch)\n",
    "- LSTM (Pytorch)\n",
    "- GRU (Pytorch)\n",
    "- Bert (HF Pretrained)\n",
    "- Roberta (HF Pretrained)\n",
    "- DistilBert (HF Pretrained)\n",
    "- Albert (HF Pretrained)\n",
    "- customized decoder only model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training, Evaluation and Prediction\n",
    "\n",
    "In this step, the original sentences are already converted into matrices, along with its labels.\n",
    "\n",
    "train each model specified in workflows and evaluate. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Tuple\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "#generates a confusion matrix between hand labelled data and model predictions\n",
    "def getConfMatrix(pred_data, actual):\n",
    "    conf_mat = confusion_matrix(actual, pred_data, labels=[0,1]) \n",
    "    micro = f1_score(actual, pred_data, average='micro') \n",
    "    macro = f1_score(actual,pred_data, average='macro')\n",
    "    sns.heatmap(conf_mat, annot = True, fmt=\".0f\", annot_kws={\"size\": 18})\n",
    "    print('F1 Micro: '+ str(micro))\n",
    "    print('F1 Macro: '+ str(macro))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import math\n",
    "import copy, os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import ray\n",
    "from ray import train, tune\n",
    "from ray.train import Checkpoint\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "global_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class myRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        h0 = torch.zeros(self.rnn.num_layers, x.size(0), self.rnn.hidden_size).to(x.device)\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "class myLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        h0 = torch.zeros(self.lstm.num_layers, x.size(0), self.lstm.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.lstm.num_layers, x.size(0), self.lstm.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "class myGRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1, bidirectional=False):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        h0 = torch.zeros(self.gru.num_layers * 2 if self.gru.bidirectional else self.gru.num_layers, x.size(0), self.gru.hidden_size).to(x.device)\n",
    "        out, _ = self.gru(x, h0)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    The init constructor checks whether the provided d_model is divisible by the number of heads (num_heads). \n",
    "    It sets up the necessary parameters and creates linear transformations for\n",
    "    query(W_q), key(W_k) and output(W_o) projections\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(self).__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    \"\"\"\n",
    "     The scaled_dot_product_attention function computes the scaled dot-product attention given the \n",
    "     query (Q), key (K), and value (V) matrices. It uses the scaled dot product formula, applies a mask if \n",
    "     provided, and computes the attention probabilities using the softmax function.\n",
    "    \"\"\"    \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output\n",
    "    \n",
    "    \"\"\"\n",
    "    The split_heads and combine_heads functions handle the splitting and combining of the attention heads.\n",
    "    They reshape the input tensor to allow parallel processing of different attention heads.\n",
    "    \"\"\"\n",
    "    def split_heads(self, x):\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "    def combine_heads(self, x):\n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "    \n",
    "    \"\"\"\n",
    "     The forward function takes input query (Q), key (K), and value (V) tensors, \n",
    "     applies linear transformations, splits them into multiple heads, performs scaled dot-product attention,\n",
    "     combines the attention heads, and applies a final linear transformation.\n",
    "    \"\"\"    \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "        \n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        return output\n",
    "class PositionWiseFeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    PositionWiseFeedForward module. It takes d_model as the input dimension and d_ff \n",
    "    as the hidden layer dimension. \n",
    "    Two linear layers (fc1 and fc2) are defined with ReLU activation in between.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    \"\"\"\n",
    "    The forward function takes an input tensor x, applies the first linear transformation (fc1), \n",
    "    applies the ReLU activation, and then applies the second linear transformation (fc2). \n",
    "    The output is the result of the second linear transformation.\n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    The constructor (__init__) initializes the PositionalEncoding module. \n",
    "    It takes d_model as the dimension of the model and max_seq_length as the maximum sequence length. \n",
    "    It computes the positional encoding matrix (pe) using sine and cosine functions.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(self).__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "    \n",
    "    \"\"\"\n",
    "    The forward function takes an input tensor x and adds the positional encoding to it. \n",
    "    The positional encoding is truncated to match the length of the input sequence (x.size(1)).\n",
    "    \"\"\"    \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "class EncoderLayer(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    The constructor (__init__) initializes the EncoderLayer module. \n",
    "    It takes hyperparameters such as d_model (model dimension), num_heads (number of attention heads), \n",
    "    d_ff (dimension of the feedforward network), and dropout (dropout rate). \n",
    "    It creates instances of MultiHeadAttention, PositionWiseFeedForward, and nn.LayerNorm. \n",
    "    Dropout is also defined as a module.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    \"\"\"\n",
    "    The forward function takes an input tensor x and a mask. \n",
    "    It applies the self-attention mechanism (self.self_attn), adds the residual connection \n",
    "    with layer normalization, applies the position-wise feedforward network (self.feed_forward),\n",
    "    and again adds the residual connection with layer normalization. \n",
    "    Dropout is applied at both the self-attention and feedforward stages.\n",
    "    The mask parameter is used to mask certain positions during the self-attention step, \n",
    "    typically to prevent attending to future positions in a sequence.\n",
    "    \"\"\"\n",
    "    def forward(self, x, mask):\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x\n",
    "class EncoderOnlyTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    The constructor (__init__) initializes the Transformer module. \n",
    "    It takes several hyperparameters, including vocabulary sizes for the source and target languages \n",
    "    (src_vocab_size and tgt_vocab_size), model dimension (d_model), number of attention heads (num_heads), \n",
    "    number of layers (num_layers), dimension of the feedforward network (d_ff), maximum sequence length \n",
    "    (max_seq_length), and dropout rate (dropout).\n",
    "    It sets up embeddings for both the encoder and decoder (encoder_embedding and decoder_embedding), \n",
    "    a positional encoding module (positional_encoding), encoder layers (encoder_layers), \n",
    "    decoder layers (decoder_layers), a linear layer (fc), and dropout.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
    "        super(self).__init__()\n",
    "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    \"\"\"\n",
    "     The generate_mask function creates masks for the source and target sequences. \n",
    "     It generates a source mask by checking if the source sequence elements are not equal to 0. \n",
    "     For the target sequence, it creates a mask by checking if the target sequence elements are not equal \n",
    "     to 0 and applies a no-peek mask to prevent attending to future positions.\n",
    "    \"\"\"\n",
    "    def generate_mask(self, src, tgt):\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
    "        seq_length = tgt.size(1)\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
    "        tgt_mask = tgt_mask & nopeak_mask\n",
    "        return src_mask, tgt_mask\n",
    "\n",
    "    \"\"\"\n",
    "    The forward function takes source (src) and target (tgt) sequences as input. \n",
    "    It generates source and target masks using the generate_mask function. \n",
    "    The source and target embeddings are obtained by applying dropout to the positional embeddings of the \n",
    "    encoder and decoder embeddings, respectively. \n",
    "    The encoder layers are then applied to the source embeddings to get the encoder output (enc_output). \n",
    "    The decoder layers are applied to the target embeddings along with the encoder output, source mask, \n",
    "    and target mask to get the final decoder output (dec_output). The output is obtained by applying a linear layer to the decoder output.\n",
    "    \"\"\"\n",
    "    def forward(self, src, tgt):\n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
    "\n",
    "        enc_output = src_embedded\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "\n",
    "        output = self.fc(enc_output)\n",
    "        return output\n",
    "\n",
    "def model_train(method: str, train_test_data: pd.DataFrame, pred_data: pd.DataFrame) -> list:\n",
    "    if method == 'SVM':\n",
    "        from sklearn.svm import SVC\n",
    "        from sklearn.pipeline import Pipeline\n",
    "        from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "        from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "        \n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(train_test_data['reviews'], train_test_data['sentiment'], test_size=0.2, random_state=42)\n",
    "        the_pipe = Pipeline([\n",
    "            ('estimator', SVC())\n",
    "        ])\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "        param_grid = {\n",
    "            'estimator__C': [0.1, 0.2, 0.3, 0.5, 1, 2, 3, 5, 10],\n",
    "            'estimator__kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "            'estimator__gamma': ['scale', 'auto']\n",
    "        }\n",
    "\n",
    "        grid_search = GridSearchCV(the_pipe, param_grid, cv=kf, n_jobs=-2, verbose=2, scoring='accuracy')\n",
    "        grid_search.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = grid_search.predict(X_test)\n",
    "        getConfMatrix(y_pred, y_test)\n",
    "\n",
    "        y_output = grid_search.predict(pred_data['reviews'])\n",
    "\n",
    "        return [f1_score(y_test, y_pred, average='micro'), y_output]\n",
    "\n",
    "    elif method == 'ELM':\n",
    "        from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "        from skelm import ELMClassifier\n",
    "        from sklearn.pipeline import Pipeline\n",
    "        from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(train_test_data['reviews'], train_test_data['sentiment'], test_size=0.2, random_state=42)\n",
    "        the_pipe = Pipeline([\n",
    "            ('estimator', ELMClassifier())\n",
    "        ]) \n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "        param_grid = {\n",
    "            'estimator__alpha': [0.00000002],\n",
    "        }\n",
    "        grid_search = GridSearchCV(the_pipe, param_grid, cv=kf, n_jobs=-2, verbose=2, scoring='accuracy')\n",
    "        grid_search.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = grid_search.predict(X_test)\n",
    "        getConfMatrix(y_pred, y_test)\n",
    "\n",
    "        y_output = grid_search.predict(pred_data['reviews'])\n",
    "\n",
    "        return [f1_score(y_test, y_pred, average='micro'), y_output]\n",
    "    \n",
    "    elif method == 'GaussianProcess':\n",
    "        from sklearn.pipeline import Pipeline\n",
    "        from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "        from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "        from sklearn.gaussian_process.kernels import RBF\n",
    "        from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(train_test_data['reviews'], train_test_data['sentiment'], test_size=0.2, random_state=42)\n",
    "        the_pipe = Pipeline([\n",
    "            ('estimator', GaussianProcessClassifier())\n",
    "        ])\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "        param_grid = {\n",
    "            'estimator__random_state' : [42]\n",
    "        }\n",
    "        grid_search = GridSearchCV(the_pipe, param_grid, cv=kf, n_jobs=-2, verbose=2, scoring='accuracy')\n",
    "        grid_search.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = grid_search.predict(X_test)\n",
    "        getConfMatrix(y_pred, y_test)\n",
    "\n",
    "        y_output = grid_search.predict(pred_data['reviews'])\n",
    "\n",
    "        return [f1_score(y_test, y_pred, average='micro'), y_output]\n",
    "\n",
    "    elif method == 'RNN':\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        from torch.utils.data import DataLoader, TensorDataset\n",
    "        from torch.optim import Adam\n",
    "        import torch\n",
    "        from ray import train, tune\n",
    "        from ray.tune.search.optuna import OptunaSearch\n",
    "        import ray\n",
    "        import optuna\n",
    "\n",
    "        def train_RNN(config: Dict[str, float]):\n",
    "            X_train, X_test, y_train, y_test = train_test_split(train_test_data['reviews'], train_test_data['sentiment'], test_size=0.2, random_state=42)\n",
    "            X_train = torch.tensor(X_train.to_numpy()).to(global_device)\n",
    "            y_train = torch.tensor(y_train.to_numpy()).to(global_device)\n",
    "            X_test = torch.tensor(X_test.to_numpy()).to(global_device)\n",
    "            y_test = torch.tensor(y_test.to_numpy()).to(global_device)\n",
    "            train_dataset = TensorDataset(X_train, y_train)\n",
    "            train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "            model = myRNN(input_size=X_train.shape[2], hidden_size=config['hidden_size'], output_size=1).to(global_device)\n",
    "            criterion = nn.BCEWithLogitsLoss().to(global_device)\n",
    "            optimizer = Adam(model.parameters(), lr=config['lr'])\n",
    "            while True: # let train.RunConfig.stop determine when to stop, each iter is one epoch\n",
    "                for i, (X , y) in enumerate(train_loader):\n",
    "                    optimizer.zero_grad()\n",
    "                    y_pred = model(X.float())\n",
    "                    loss = criterion(y_pred, y.float().view(-1, 1))\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                y_pred = model(X_test.float())\n",
    "                y_pred = [1 if pred > 0.5 else 0 for pred in y_pred]\n",
    "                accuracy = accuracy_score(y_test, y_pred)\n",
    "                train.report({\"acc_report\": accuracy})\n",
    "        search_space = {\n",
    "            'hidden_size': tune.randint(50, 200),\n",
    "            'lr': tune.loguniform(1e-4, 1e-1),\n",
    "            'batch_size': tune.choice([32, 64, 128])\n",
    "        }\n",
    "\n",
    "        algo = OptunaSearch()\n",
    "        tuner = tune.Tuner(\n",
    "            train_RNN,\n",
    "            tune_config=tune.TuneConfig(\n",
    "                num_samples=10,\n",
    "                metric='acc_report',\n",
    "                mode='max',\n",
    "                search_alg=algo\n",
    "            ),\n",
    "            run_config=train.RunConfig(\n",
    "                name='rnn_tuner',\n",
    "                storage_path=os.path.join(os.getcwd(),'log', 'optuna_storage'),\n",
    "                stop={\"training_iteration\": 100,\n",
    "                      \"acc_report\": 0.95},\n",
    "            ),\n",
    "            param_space=search_space,\n",
    "        )\n",
    "        results = tuner.fit()\n",
    "        print(\"Best config is:\", results.get_best_result().config)\n",
    "\n",
    "        # train agian with the best config\n",
    "        pass\n",
    "\n",
    "    elif method == 'LSTM':\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        from torch.utils.data import DataLoader, TensorDataset\n",
    "        from torch.optim import Adam\n",
    "        import torch\n",
    "        from ray import train, tune\n",
    "        from ray.tune.search.optuna import OptunaSearch\n",
    "        import ray\n",
    "        import optuna\n",
    "\n",
    "        def train_LSTM(config: Dict[str, float]):\n",
    "            X_train, X_test, y_train, y_test = train_test_split(train_test_data['reviews'], train_test_data['sentiment'], test_size=0.2, random_state=42)\n",
    "            X_train = torch.tensor(X_train.to_numpy()).to(global_device)\n",
    "            y_train = torch.tensor(y_train.to_numpy()).to(global_device)\n",
    "            X_test = torch.tensor(X_test.to_numpy()).to(global_device)\n",
    "            y_test = torch.tensor(y_test.to_numpy()).to(global_device)\n",
    "            train_dataset = TensorDataset(X_train, y_train)\n",
    "            train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "            model = myLSTM(input_size=X_train.shape[2], hidden_size=config['hidden_size'], output_size=1).to(global_device)\n",
    "            criterion = nn.BCEWithLogitsLoss().to(global_device)\n",
    "            optimizer = Adam(model.parameters(), lr=config['lr'])\n",
    "            while True: # let train.RunConfig.stop determine when to stop, each iter is one epoch\n",
    "                for i, (X , y) in enumerate(train_loader): # one batch\n",
    "                    optimizer.zero_grad()\n",
    "                    y_pred = model(X.float())\n",
    "                    loss = criterion(y_pred, y.float().view(-1, 1))\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                y_pred = model(X_test.float())\n",
    "                y_pred = [1 if pred > 0.5 else 0 for pred in y_pred]\n",
    "                accuracy = accuracy_score(y_test, y_pred)\n",
    "                train.report({\"acc_report\": accuracy})\n",
    "        search_space = {\n",
    "            'hidden_size': tune.randint(50, 200),\n",
    "            'lr': tune.loguniform(1e-4, 1e-1),\n",
    "            'batch_size': tune.choice([32, 64, 128])\n",
    "        }\n",
    "\n",
    "        algo = OptunaSearch()\n",
    "        tuner = tune.Tuner(\n",
    "            train_LSTM,\n",
    "            tune_config=tune.TuneConfig(\n",
    "                num_samples=10,\n",
    "                metric='acc_report',\n",
    "                mode='max',\n",
    "                search_alg=algo\n",
    "            ),\n",
    "            run_config=train.RunConfig(\n",
    "                name='lstm_tuner',\n",
    "                storage_path=os.path.join(os.getcwd(),'log', 'optuna_storage'),\n",
    "                stop={\"training_iteration\": 333,\n",
    "                      \"acc_report\": 0.95},\n",
    "            ),\n",
    "            param_space=search_space,\n",
    "        )\n",
    "        results = tuner.fit()\n",
    "        print(\"Best config is:\", results.get_best_result().config)\n",
    "\n",
    "        # train agian with the best config\n",
    "        pass\n",
    "\n",
    "    elif method == 'GRU':\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        from torch.utils.data import DataLoader, TensorDataset\n",
    "        from torch.optim import Adam\n",
    "        import torch\n",
    "        from ray import train, tune\n",
    "        from ray.tune.search.optuna import OptunaSearch\n",
    "        import ray\n",
    "        import optuna\n",
    "\n",
    "        def train_GRU(config: Dict[str, float]):\n",
    "            X_train, X_test, y_train, y_test = train_test_split(train_test_data['reviews'], train_test_data['sentiment'], test_size=0.2, random_state=42)\n",
    "            X_train = torch.tensor(X_train.to_numpy()).to(global_device)\n",
    "            y_train = torch.tensor(y_train.to_numpy()).to(global_device)\n",
    "            X_test = torch.tensor(X_test.to_numpy()).to(global_device)\n",
    "            y_test = torch.tensor(y_test.to_numpy()).to(global_device)\n",
    "            train_dataset = TensorDataset(X_train, y_train)\n",
    "            train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "            model = myGRU(input_size=X_train.shape[2], hidden_size=config['hidden_size'], output_size=1, bidirectional=config['bidirectional']).to(global_device)\n",
    "            criterion = nn.BCEWithLogitsLoss().to(global_device)\n",
    "            optimizer = Adam(model.parameters(), lr=config['lr'])\n",
    "            while True: # let train.RunConfig.stop determine when to stop, each iter is one epoch\n",
    "                for i, (X , y) in enumerate(train_loader): # one batch\n",
    "                    optimizer.zero_grad()\n",
    "                    y_pred = model(X.float())\n",
    "                    loss = criterion(y_pred, y.float().view(-1, 1))\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                y_pred = model(X_test.float())\n",
    "                y_pred = [1 if pred > 0.5 else 0 for pred in y_pred]\n",
    "                accuracy = accuracy_score(y_test, y_pred)\n",
    "                train.report({\"acc_report\": accuracy})\n",
    "        search_space = {\n",
    "            'hidden_size': tune.randint(50, 200),\n",
    "            'lr': tune.loguniform(1e-4, 1e-1),\n",
    "            'batch_size': tune.choice([32, 64, 128]),\n",
    "            'bidirectional': tune.choice([True, False])\n",
    "        }\n",
    "\n",
    "        algo = OptunaSearch()\n",
    "        tuner = tune.Tuner(\n",
    "            train_GRU,\n",
    "            tune_config=tune.TuneConfig(\n",
    "                num_samples=10,\n",
    "                metric='acc_report',\n",
    "                mode='max',\n",
    "                search_alg=algo\n",
    "            ),\n",
    "            run_config=train.RunConfig(\n",
    "                name='gru_tuner',\n",
    "                storage_path=os.path.join(os.getcwd(),'log', 'optuna_storage'),\n",
    "                stop={\"training_iteration\": 333,\n",
    "                      \"acc_report\": 0.95},\n",
    "            ),\n",
    "            param_space=search_space,\n",
    "        )\n",
    "        results = tuner.fit()\n",
    "        print(\"Best config is:\", results.get_best_result().config)\n",
    "\n",
    "        # train agian with the best config\n",
    "        pass\n",
    "\n",
    "    elif method == 'BERT' or method == \"DistilBert\" or method == \"Roberta\":\n",
    "        if method == 'BERT':\n",
    "            PRETRAIN_HF_NAME = 'bert-base-cased'\n",
    "            FINETUNED = 'bert'\n",
    "        elif method == 'DistilBert':\n",
    "            PRETRAIN_HF_NAME = 'distilbert-base-cased'\n",
    "            FINETUNED = 'distilbert'\n",
    "        elif method == 'Roberta':\n",
    "            PRETRAIN_HF_NAME = 'roberta-base'\n",
    "            FINETUNED = 'roberta'\n",
    "        \n",
    "        \n",
    "    elif method == 'costumized':\n",
    "        raise NotImplementedError()\n",
    "    else:\n",
    "        raise ValueError('method not supported')\n",
    "    \n",
    "\n",
    "for taski in range(len(taskflows)):\n",
    "\n",
    "    # if we use ML based model, we literally fall back to BOW.\n",
    "    if taskflows[taski] & F.MODEL_SVM:\n",
    "        container_traintest[taski].append(model_train('SVM', container_traintest[taski][0], container_pred[taski][0]))\n",
    "    elif taskflows[taski] & F.MODEL_ELM:\n",
    "        container_traintest[taski].append(model_train('ELM', container_traintest[taski][0], container_pred[taski][0]))\n",
    "    elif taskflows[taski] & F.MODEL_GAUSSIANPROCESS:\n",
    "        container_traintest[taski].append(model_train('GaussianProcess', container_traintest[taski][0], container_pred[taski][0]))\n",
    "    elif taskflows[taski] & F.MODEL_RNN:\n",
    "        container_traintest[taski].append(model_train('RNN', container_traintest[taski][0], container_pred[taski][0]))\n",
    "    elif taskflows[taski] & F.MODEL_LSTM:\n",
    "        container_traintest[taski].append(model_train('LSTM', container_traintest[taski][0], container_pred[taski][0]))\n",
    "    elif taskflows[taski] & F.MODEL_GRU:\n",
    "        container_traintest[taski].append(model_train('GRU', container_traintest[taski][0], container_pred[taski][0]))\n",
    "    elif taskflows[taski] & F.MODEL_BERT:\n",
    "        container_traintest[taski].append(model_train('BERT', container_traintest[taski][0], container_pred[taski][0]))\n",
    "    elif taskflows[taski] & F.MODEL_DISTILBERT:\n",
    "        container_traintest[taski].append(model_train('DistilBert', container_traintest[taski][0], container_pred[taski][0]))\n",
    "    elif taskflows[taski] & F.MODEL_ROBERTA:\n",
    "        container_traintest[taski].append(model_train('Roberta', container_traintest[taski][0], container_pred[taski][0]))\n",
    "    elif taskflows[taski] & F.MODEL_COSTUMIZED:\n",
    "        container_traintest[taski].append(model_train('costumized', container_traintest[taski][0], container_pred[taski][0]))\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentiment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
